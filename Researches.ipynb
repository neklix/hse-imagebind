{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b804e9-796b-4cbc-a967-49f7e5d2b029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./ImageBind/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46767fa-21f8-4943-bed8-7a8e13d8a426",
   "metadata": {},
   "source": [
    "Пробуем добавить очки собакам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fbc8f-8dee-44f4-97ce-55469715b75c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imagebind\n",
    "import torch\n",
    "from diffusers import StableUnCLIPImg2ImgPipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-unclip\", torch_dtype=torch.float32\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "model = imagebind.imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for img_w in [0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "    with torch.no_grad():\n",
    "        paths = [\"assets/orig_scottish.png\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                    paths, device\n",
    "                ),\n",
    "                imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                    [\"sunglasses\"], device\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            img_w * embeddings[imagebind.imagebind_model.ModalityType.VISION]\n",
    "            + (1 - img_w) * embeddings[imagebind.imagebind_model.ModalityType.TEXT]\n",
    "        ).view(1, -1)\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/dogsunglasses_v2_orig{img_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e397a39-5e8d-416a-ae27-09e6714a2007",
   "metadata": {},
   "source": [
    "Text+Text -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2fbe4-8832-450e-bd42-e5af5b59a548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img_w in [0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "    with torch.no_grad():\n",
    "        paths = [\"assets/orig_scottish.png\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                    [\"dog\", \"sunset\"], device\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            img_w * embeddings[imagebind.imagebind_model.ModalityType.TEXT][0]\n",
    "            + (1 - img_w) * embeddings[imagebind.imagebind_model.ModalityType.TEXT][1]\n",
    "        ).view(1, -1)\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/texttext_v2_dog{img_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9dbcc0-0d16-417a-9452-c803150ff6e9",
   "metadata": {},
   "source": [
    "Text -> Image\n",
    "\n",
    "Image -> Image\n",
    "\n",
    "Audio -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668da4f6-b961-482e-b01d-46ee1b49c643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    paths = [\"assets/orig_scottish.png\"]\n",
    "    embeddings = model.forward(\n",
    "        {\n",
    "            imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                [\"sunset\"], device\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    embeddings = (\n",
    "        embeddings[imagebind.imagebind_model.ModalityType.TEXT]\n",
    "    ).view(1, -1)\n",
    "    images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "    images[0].save(f\"experiments/text2img.png\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    paths = [\"assets/orig_scottish.png\"]\n",
    "    embeddings = model.forward(\n",
    "        {\n",
    "            imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                paths, device\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    embeddings = (\n",
    "        embeddings[imagebind.imagebind_model.ModalityType.VISION]\n",
    "    ).view(1, -1)\n",
    "    images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "    images[0].save(f\"experiments/img2img.png\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    paths = [\"assets/wave.wav\"]\n",
    "    embeddings = model.forward(\n",
    "        {\n",
    "            imagebind.imagebind_model.ModalityType.AUDIO: imagebind.data.load_and_transform_audio_data(\n",
    "                paths, device\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    embeddings = (\n",
    "        embeddings[imagebind.imagebind_model.ModalityType.AUDIO]\n",
    "    ).view(1, -1)\n",
    "    images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "    images[0].save(f\"experiments/audio2img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2097d2-ef37-4d2c-bf67-78cd8e6ef4fd",
   "metadata": {},
   "source": [
    "Image+Image -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d949bef-4635-404b-a3c8-e6cd42b4fff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img_w in [0.2, 0.4, 0.6, 0.8]:\n",
    "    with torch.no_grad():\n",
    "        paths = [\"assets/orig_scottish.png\", \"assets/Drake_-_For_All_The_Dogs.png\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                    paths, device\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            img_w * embeddings[imagebind.imagebind_model.ModalityType.VISION][0]\n",
    "            + (1 - img_w) * embeddings[imagebind.imagebind_model.ModalityType.VISION][1]\n",
    "        ).view(1, -1)\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/imgimg2img_v2_orig{img_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed22d06-4a88-4e8f-be8c-8a83a1ada2b9",
   "metadata": {},
   "source": [
    "Image+Text -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84959b8-8346-44ca-888c-334715d864a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_w in [0.2, 0.4, 0.6, 0.8]:\n",
    "    with torch.no_grad():\n",
    "        paths = [\"assets/orig_scottish.png\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                    paths, device\n",
    "                ),\n",
    "                imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                    [\"sunset\"], device\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            img_w * embeddings[imagebind.imagebind_model.ModalityType.VISION]\n",
    "            + (1 - img_w) * embeddings[imagebind.imagebind_model.ModalityType.TEXT]\n",
    "        ).view(1, -1)\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/imgtext2img_v2_orig{img_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b548b7e-5bcd-4043-bd7c-928876762195",
   "metadata": {},
   "source": [
    "Try to remove dog by subtracting \"dog\" vector from image vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcced0d-4c7a-48ba-a22a-c196f3ff04b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img_w in [0.5, 0.75, 1.0, 1.25, 1.5]:\n",
    "    with torch.no_grad():\n",
    "        paths = [\"assets/orig_scottish.png\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                    paths, device\n",
    "                ),\n",
    "                imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                    [\"dog\"], device\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            embeddings[imagebind.imagebind_model.ModalityType.VISION]\n",
    "            - img_w * embeddings[imagebind.imagebind_model.ModalityType.TEXT]\n",
    "        ).view(1, -1)\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/substr_imgtext2img_v2_text{img_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796a39c-8b28-47ac-b700-e7834ff456ae",
   "metadata": {},
   "source": [
    "Image+Audio+Text -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01160543-fd73-4dd6-8846-0eb19627b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_w in [0.25, 0.5, 0.75]:\n",
    "    for text_w in [0.25, 0.5, 0.75]:\n",
    "        for audio_w in [0.25, 0.5, 0.75]:\n",
    "            with torch.no_grad():\n",
    "                paths = [\"assets/orig_scottish.png\"]\n",
    "                audio_paths = [\"assets/wave.wav\"]\n",
    "                embeddings = model.forward(\n",
    "                    {\n",
    "                        imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                            paths, device\n",
    "                        ),\n",
    "                        imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                            [\"sunset\"], device\n",
    "                        ),\n",
    "                        imagebind.imagebind_model.ModalityType.AUDIO: imagebind.data.load_and_transform_audio_data(\n",
    "                            audio_paths, device\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "                embeddings = (\n",
    "                    img_w * embeddings[imagebind.imagebind_model.ModalityType.VISION]\n",
    "                    + text_w * embeddings[imagebind.imagebind_model.ModalityType.TEXT]\n",
    "                    + audio_w * embeddings[imagebind.imagebind_model.ModalityType.AUDIO]\n",
    "                )\n",
    "                images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "                images[0].save(f\"experiments/audiotextimg_v2_{img_w}_{text_w}_{audio_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eab631-e0e6-4308-b846-e33ad95b294c",
   "metadata": {},
   "source": [
    "Text+Audio -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491eb25-1a04-4283-bcc4-0b660cf2ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_w in [0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "    with torch.no_grad():\n",
    "        paths = [\"assets/wave.wav\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.AUDIO: imagebind.data.load_and_transform_audio_data(\n",
    "                    paths, device\n",
    "                ),\n",
    "                imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                    [\"sunset\"], device\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            img_w * embeddings[imagebind.imagebind_model.ModalityType.AUDIO]\n",
    "            + (1 - img_w) * embeddings[imagebind.imagebind_model.ModalityType.TEXT]\n",
    "        ).view(1, -1)\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/audiotext2img_v2_audio{img_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc76521-6d93-45f7-8681-dfcbd22d0585",
   "metadata": {},
   "source": [
    "Audio+Image -> Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f89018-3485-4309-810b-1ba2225cc0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_w in [0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "    with torch.no_grad():\n",
    "        audio_paths = [\"assets/wave.wav\"]\n",
    "        img_paths = [\"assets/orig_scottish.png\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.AUDIO: imagebind.data.load_and_transform_audio_data(\n",
    "                    audio_paths, device\n",
    "                ),\n",
    "                imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                    img_paths, device\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            img_w * embeddings[imagebind.imagebind_model.ModalityType.AUDIO]\n",
    "            + (1 - img_w) * embeddings[imagebind.imagebind_model.ModalityType.VISION]\n",
    "        ).view(1, -1)\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/audioimg2img_v2_audio{img_w}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6b5aa-94ff-4847-82ea-8845505251ae",
   "metadata": {},
   "source": [
    "Trying to remove sunglasses from the dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebea83-944b-483a-b2b2-88efa6e1885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_w in [0.2, 0.4, 0.5, 0.6, 0.8]:\n",
    "    with torch.no_grad():\n",
    "        paths = [\"assets/dogsunglasses_v2_orig0.6.png\"]\n",
    "        embeddings = model.forward(\n",
    "            {\n",
    "                imagebind.imagebind_model.ModalityType.VISION: imagebind.data.load_and_transform_vision_data(\n",
    "                    paths, device\n",
    "                ),\n",
    "                imagebind.imagebind_model.ModalityType.TEXT: imagebind.data.load_and_transform_text(\n",
    "                    [\"sunglasses\"], device\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        embeddings = (\n",
    "            embeddings[imagebind.imagebind_model.ModalityType.VISION]\n",
    "            - (1 - img_w) * embeddings[imagebind.imagebind_model.ModalityType.TEXT]\n",
    "        ) / img_w\n",
    "        images = pipe(image_embeds=embeddings, num_inference_steps=30).images\n",
    "        images[0].save(f\"experiments/substr_sunglasses2img_v2_text{img_w}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
